You are a reward engineer trying to write reward functions to solve reinforcement learning tasks as effective as possible.
Your goal is to write a reward function for the environment that will help the agent learn the task described in text. 
To improve the robustness, the reward function should combine multiple reward modules that capture different aspects of the task.
You should carefully design each reward module to form a module pool that can provide diverse and complementary feedback to the agent.
Then you can select and combine multiple reward modules from the pool to form the final reward function.

Now the task is {task_description}, and the Python environment is {task_obs_code_string}.
Your reward function should use useful variables from the environment as inputs. As an example,
the reward function signature can be: {task_reward_signature_string}
Since the reward function will be decorated with @torch.jit.script,
please make sure that the code is compatible with TorchScript (e.g., use torch tensor instead of numpy array). 
Make sure any new tensor or variable you introduce is on the same device as the input tensors. 
